We are building solid foundations over which we will add latest llm tech.

This phase includes:
- Andrej Karpathy's series
- Reading Foundational LLM papers
    - Transformers - https://arxiv.org/abs/1706.03762
    - NN based Language model - https://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf
    - Word Embeddings - https://arxiv.org/abs/1301.3781
    - BERT - https://arxiv.org/abs/1810.04805
    - GPT - https://cdn.openai.com/research-covers/language-models/language_models_are_few_shot_learners.pdf
    - GPT-2 - https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
    - A traditional transformer survey - https://arxiv.org/abs/2106.04554
    - ROPE - https://arxiv.org/abs/2104.09864
    - LORA - https://arxiv.org/abs/2106.09685
    - https://arxiv.org/pdf/2405.14159 ???
    - https://arxiv.org/pdf/2402.14905 ???

- Reading Modern Survey papers 
    - https://arxiv.org/pdf/2407.06204
    - https://www.labellerr.com/blog/exploring-architectures-and-configurations-for-large-language-models-llms/
    - https://arxiv.org/abs/2402.06196v3
    - https://arxiv.org/abs/2508.09834
    - https://arxiv.org/pdf/2411.03350