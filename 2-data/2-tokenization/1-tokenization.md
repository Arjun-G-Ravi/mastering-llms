# Tokenization
- Tokenization is the process of breaking text into smaller pieces called tokens before feeding it to a Large Language Model (LLM).
- It is a crucial step, as LLMs operate on these tokens rather than raw text. If this is wrong, nothing else will pretty much matter.
- Modern LLMs (like GPT) use subword tokenization (e.g., Byte-Pair Encoding, WordPiece).