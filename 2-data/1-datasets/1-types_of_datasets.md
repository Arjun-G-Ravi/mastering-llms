# Types of Datasets
Pretraining Datasets:
    - Large, diverse text corpora used for initial “general” training.
    - Wikipedia, Fineweb, Common Crawl.
    - Tinystory is a good dataset for small-scale experiments.
Fine-tuning Datasets:
    - More specific text collections tailored to a task like instruction following, Q&A, coding, etc.
    - Eg: Instruction-following datasets like Alpaca

- Dataset quality and diversity > dataset quantity, especially for fine-tuning and smaller models.